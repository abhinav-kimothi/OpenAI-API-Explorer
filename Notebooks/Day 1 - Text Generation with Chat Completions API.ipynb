{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/Day 1 Header (Text).png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Day 1\n",
    "\n",
    "Today we will \n",
    "\n",
    "- Introduce LLMs\n",
    "- Understand the available OpenAI LLMs\n",
    "- Look at the OpenAI Chat Messages (Prompt) Structure\n",
    "- Study Chat Completions API Parameters\n",
    "- Decode the Chat Completions Response Object\n",
    "- Learn how to Stream responses\n",
    "- Look at JSON Response format\n",
    "- Understand Reproducibility\n",
    "- Learn about managing Tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction to Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\"><b>Generative AI, and LLMs specifically, is a General Purpose Technology that is useful for a variety of applications</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px;\"><i>\"LLMs can be, generally, thought of as a next word prediction model\"</i></span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"><b>What is an LLM?</b></span>\n",
    "\n",
    "- LLMs are __machine learning models__ that have learned from __massive datasets__ of human-generated content, finding statistical patterns to replicate human-like abilities.\n",
    "\n",
    "- __Foundation models__, also known as base models, have been trained on trillions of words for weeks or months using extensive compute power. These models have __billions of parameters__, which represent their memory and enable sophisticated tasks.\n",
    "\n",
    "- __Interacting with LLMs differs from traditional programming paradigms. Instead of formalized code syntax, you provide natural language prompts to the models__.\n",
    "\n",
    "- When you pass a __prompt__ to the model, it predicts the next words and generates a __completion__. This process is known as __inference__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px; color: blue\"> <b>Prompts, Completions and Inference!</b></span>\n",
    "\n",
    "<img src=\"../Assets/Images/LLM Inference.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Recommendation__ : An excellent course \"[Generative AI using Large Language Models](https://www.deeplearning.ai/courses/generative-ai-with-llms/)\" is offered by DeepLearning.ai and AWS via Coursera.\n",
    "\n",
    "I've prepared notes on this course that you can download \n",
    "\n",
    "<a href=\"https://abhinavkimothi.gumroad.com/l/GenAILLM\"><img src=\"../Assets/Images/Coursera Course Notes.png\" width=400></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available OpenAI LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px; color: blue\"> <b>GPT-4</b></span>\n",
    "\n",
    "__<u>(Production)</u>__\n",
    "\n",
    "<u>Name        | Context Window    | Cut-off date      | Snapshot</u>\n",
    "\n",
    "__gpt-4__       | 8,192 tokens      | Up to Sep 2021    | gpt-4-0613\n",
    "\n",
    "__gpt-4-32k__   | 32,768 tokens     | Up to Sep 2021    | gpt-4-32k-0613\n",
    "\n",
    "<b><u>(Preview)</b></u>\n",
    "\n",
    "__gpt-4-turbo-preview__     | 128,000 tokens | Up to Dec 2023    | gpt-4-1106-preview\n",
    "\n",
    "__gpt-4-vision-preview__    | 128,000 tokens | Up to Apr 2023    | gpt-4-1106-vision-preview\n",
    "\n",
    "_Note : gpt-4-1106-vision-preview is a multi-modal model that works on both text and images_\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"> <b>GPT-3.5</b></span>\n",
    "\n",
    "\n",
    "__gpt-3.5-turbo__ | 16,385 tokens | Up to Sep 2021 | gpt-3.5-turbo-1106\n",
    "\n",
    "__gpt-3.5-turbo-instruct__ | 4,096 tokens | Up to Sep 2021\n",
    "\n",
    "---\n",
    "\n",
    "For more details, visit the official documentation -> https://platform.openai.com/docs/models\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 14px; color: orange\">__IMP__ : __\"model\"__ is passed as a parameter in the chat completions API</span>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Chat Messages (Prompt) Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 16px;\"><u>Role</u></span> : OpenAI allows for *three* roles/personas - \n",
    "1. __System__ : The overarching constraints/definitions/intructions of the system that the LLM should \"remember\"\n",
    "2. __User__ : Any instruction a user wants to pass to the LLM\n",
    "3. __Assistant__ : The response from the LLM\n",
    "\n",
    "<span style=\"font-size: 16px;\"><u>Content</u></span> : Any message or \"prompt\" of these personas are passed as \"Content\"\n",
    "\n",
    "\n",
    "\n",
    "__Why is this important?__ : Makes it easier to adapt an LLM to store conversation history.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 14px; color: orange\">__IMP__ : __\"role\"__ and __\"content\"__ are passed as a dictionary in the __\"messages\"__ parameter in the chat completions API </span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall from Day 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai --quiet #You can remove '--quiet' to see the installation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import Libraries ####\n",
    "import openai #OpenAI python library\n",
    "from openai import OpenAI #OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the Chat Completions API. \n",
    "\n",
    "Let's talk to the LLM about the beautiful game of Cricket.\n",
    "\n",
    "Remember \"System\" is the overarching instruction that you can give to the LLM. Here let's ask the LLM to be a helpful assistant who has a knowledge of cricket.\n",
    "\n",
    "Then will ask it a question about cricket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"}, ### The overarching instruction\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"} ### Our Question\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the gpt-3.5-turbo model. This is the same model used in the free version of ChatGPT.\n",
    "\n",
    "Let's look at the response that the LLM generated. We'll look at the response object later in detail. The response text is found under choices->message->content in the response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia won their first Cricket World Cup in 1987. The tournament was held jointly in India and Pakistan, and Australia emerged as the champions by defeating England in the final.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We got a response from the LLM. The response also seems fairly accurate. \n",
    "\n",
    "Remember that there is no guarantee that the response will be factually correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask a follow up question - \"How much did they score?\"\n",
    "\n",
    "This question implies that we are chatting with the LLM and the LLM has the context of the previously asked questions and responses. To the API we will have to pass all of the previous prompts and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the 1987 Cricket World Cup final, Australia scored 253 runs for the loss of five wickets in their allotted 50 overs.England, in response, scored\n",
      "246 runs for the loss of eight wickets, and Australia won by 7 runs.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"}, ### The overarching instruction\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"}, ### Our Question\n",
    "    {\"role\": \"assistant\", \"content\": \"Australia won their first Cricket World Cup in the year 1987. They defeated England in the final to clinch their maiden title in the tournament.\"}, ### LLM Response\n",
    "    {\"role\": \"user\", \"content\": \"How much did they score?\"} ### Our followup question\n",
    "  ]\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "print(textwrap.fill(response.choices[0].message.content,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can see that model considers all the previous context while answering the latest question.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how prompts and responses work, let's deep dive into the API parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion API Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\"><b>\"model\"</b> and <b>\"messages\"</b> are the two required API parameters</span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"> There are several other optional parameters that help configure the response</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__n__ : Number of responses you want the LLM to generate for the instruction\n",
    "\n",
    "__max_tokens__ : Maximum number of tokens you want to restrict the Inference to (This includes both the prompt/messages and the completion)\n",
    "\n",
    "__temperature__ : Temperature controls the \"randomness\" of the responses. Higher value increases the randomness; lower value makes the output deterministic *(value between 0 and 2)*\n",
    "\n",
    "__top_p__ : The model considers the results of the tokens with top_p probability mass *(value between 0 and 1)*\n",
    "\n",
    "\n",
    "<img src=\"../Assets/Images/Temperature - Top P.png\" width=600>\n",
    "\n",
    "<span style=\"font-size: 14px; color: orange\"> __IMP__ : It is recommended to configure either one of \"temperature\" and \"top_p\" but not both</span>\n",
    "\n",
    "\n",
    "__frequency_penalty__ : Penalize new tokens based on their existing frequency in the text so far *(Value between -2 and 2)*\n",
    "\n",
    "__presence_penalty__ : Penalize new tokens based on whether they appear in the text so far *(Value between -2 and 2)*\n",
    "\n",
    "__logprobs__ : Flag to return log probability of the generated tokens *(True/False)*\n",
    "\n",
    "__logit_bias__ : Parameter to control the presence of particular tokens in the output *(Value between -100 and 100)*\n",
    "\n",
    "__response_format__ : Response of the model can be requested in a particular format *(Currently : JSON and Text)*\n",
    "\n",
    "__seed__ : Beta feature for reproducible outputs (setting a seed value may produce the same output repeatedly)\n",
    "\n",
    "__stop__ : End of Sequence tokens that will stop the generation\n",
    "\n",
    "__stream__ : To receive partial message deltas *(True/False)*\n",
    "\n",
    "__user__ : ID representing end user (This helps OpenAI detect abuse. May be mandatory for higher rate limits)\n",
    "\n",
    "__tools__ : used in function calling\n",
    "\n",
    "__tool_choice__ : used in function calling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function for calling the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_call(model:str=\"gpt-3.5-turbo\",prompt:str=\"Have I provided any input\",n:int=1,max_tokens:int=100,temperature:float=0.5,presence_penalty:float=0):\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "       {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    "    presence_penalty=presence_penalty,\n",
    "    n=n\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    if len(response.choices)>1:\n",
    "        output=''\n",
    "        for i in range(0,len(response.choices)):\n",
    "            output+='\\n\\n-- n = '+str(i+1)+' ------\\n\\n'+str(response.choices[i].message.content)\n",
    "    else:\n",
    "        output=response.choices[0].message.content\n",
    "        \n",
    "    \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- n = 1 ------\n",
      "\n",
      "\"Unlocking the Power of OpenAI: A Hands-On Workshop on Utilizing the OpenAI API\"\n",
      "\n",
      "-- n = 2 ------\n",
      "\n",
      "\"Unlocking Innovation with OpenAI: A Hands-On Workshop on Leveraging the OpenAI API\"\n"
     ]
    }
   ],
   "source": [
    "print(gpt_call(prompt=\"Write a title for a workshop on openai API\",n=2,temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- n = 1 ------\n",
      "\n",
      "\"Unlocking the Power of OpenAI: A Hands-On Workshop on Harnessing the OpenAI API\"\n",
      "\n",
      "-- n = 2 ------\n",
      "\n",
      "\"Unlocking the Power of OpenAI: A Hands-On Workshop on Harnessing the OpenAI API\"\n",
      "\n",
      "-- n = 3 ------\n",
      "\n",
      "\"Unlocking the Power of OpenAI: A Hands-On Workshop on Harnessing the OpenAI API\"\n"
     ]
    }
   ],
   "source": [
    "print(gpt_call(prompt=\"Write a title for a workshop on openai API\",n=3,temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kim/Desktop/Github/OpenAI-Marvels/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def text_to_uppercase(model,n,max_tokens,temperature,presence_penalty,prompt):\n",
    "    return str(type(n)) + str(type(temperature))\n",
    "\n",
    "model=gr.Radio([\"gpt-4\",\"gpt-3.5-turbo\"], label=\"Select Model\")\n",
    "n=gr.Radio([1,2,3], label=\"Number of Responses\")\n",
    "max_tokens=gr.Slider(minimum=10, maximum=500, label=\"Maximum Tokens\")\n",
    "temperature=gr.Slider(minimum=0.0, maximum=1.0, label=\"Temperature\")\n",
    "prompt=gr.Text(label=\"Prompt\")\n",
    "\n",
    "\n",
    "iface = gr.Interface(fn=gpt_call, inputs=[model,prompt,n,max_tokens,temperature], outputs=\"text\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding the Chat Completions Response Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Australia won their first Cricket World Cup in the year 1987. They defeated England in the final to clinch their maiden title in the tournament.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How much did they score?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "     \"id\": \"chatcmpl-9A34Xjkwt9ztugTdtpeBdizGIfpBG\",\n",
      "     \"choices\": [\n",
      "          {\n",
      "               \"finish_reason\": \"stop\",\n",
      "               \"index\": 0,\n",
      "               \"logprobs\": null,\n",
      "               \"message\": {\n",
      "                    \"content\": \"Australia scored 253 for 5 in their 50 overs in the final of the 1987 Cricket World Cup against England. They secured victory by 7 runs via the D/L method after rain interruption.\",\n",
      "                    \"role\": \"assistant\",\n",
      "                    \"function_call\": null,\n",
      "                    \"tool_calls\": null\n",
      "               }\n",
      "          }\n",
      "     ],\n",
      "     \"created\": 1712182117,\n",
      "     \"model\": \"gpt-3.5-turbo-0125\",\n",
      "     \"object\": \"chat.completion\",\n",
      "     \"system_fingerprint\": \"fp_b28b39ffa8\",\n",
      "     \"usage\": {\n",
      "          \"completion_tokens\": 42,\n",
      "          \"prompt_tokens\": 77,\n",
      "          \"total_tokens\": 119\n",
      "     }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print((response.model_dump_json(indent=5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/Chat Completion Object.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In\n",
      " the\n",
      " final\n",
      " of\n",
      " the\n",
      " \n",
      "198\n",
      "7\n",
      " Cricket\n",
      " World\n",
      " Cup\n",
      ",\n",
      " Australia\n",
      " scored\n",
      " \n",
      "253\n",
      " runs\n",
      " for\n",
      " the\n",
      " loss\n",
      " of\n",
      " \n",
      "5\n",
      " w\n",
      "ickets\n",
      " in\n",
      " their\n",
      " allotted\n",
      " \n",
      "50\n",
      " overs\n",
      ".\n",
      " England\n",
      ",\n",
      " in\n",
      " response\n",
      ",\n",
      " fell\n",
      " short\n",
      " by\n",
      " \n",
      "7\n",
      " runs\n",
      " as\n",
      " they\n",
      " were\n",
      " bow\n",
      "led\n",
      " out\n",
      " for\n",
      " \n",
      "246\n",
      " in\n",
      " \n",
      "47\n",
      ".\n",
      "2\n",
      " overs\n",
      ".\n",
      " Australia\n",
      " emerged\n",
      " victorious\n",
      " in\n",
      " a\n",
      " thrilling\n",
      " final\n",
      " to\n",
      " claim\n",
      " their\n",
      " first\n",
      " World\n",
      " Cup\n",
      " title\n",
      ".\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Australia won their first Cricket World Cup in the year 1987. They defeated England in the final to clinch their maiden title in the tournament.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How much did they score?\"}\n",
    "  ],\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "  print(chunk.choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"generate the entire text for a blog on a cricket match. \\\"Title\\\" is a catchy and attractive title of the blog. The \\\"Heading\\\" is the heading for each point in the blog and the \\\"Body\\\" is the text for that heading.Output in a json structure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Title\": \"Thrilling Encounter: A Recap of the Cricket Match Between India and Australia\",\n",
      "  \"Heading\": [\n",
      "    {\n",
      "      \"Heading\": \"Introduction\",\n",
      "      \"Body\": \"The recent cricket match between India and Australia was a highly anticipated one, with fans on the edge of their seats throughout the game. Both teams were in top form, and it was set to be a thrilling encounter from the very beginning.\"\n",
      "    },\n",
      "    {\n",
      "      \"Heading\": \"Match Overview\",\n",
      "      \"Body\": \"Australia won the toss and elected to bat first, putting up a formidable total of 300 runs for the loss of 5 wickets in their 50 overs. India started off their chase well, with the openers putting up a solid partnership. However, they stumbled in the middle overs and found themselves in a tough situation.\"\n",
      "    },\n",
      "    {\n",
      "      \"Heading\": \"Key Moments\",\n",
      "      \"Body\": \"One of the key moments in the match was when India's captain, Virat Kohli, was dismissed for a crucial 70 runs. The pressure was on the middle-order batsmen to deliver, and they did not disappoint. Hardik Pandya played a brilliant innings, scoring a quickfire 50 to keep India's hopes alive.\"\n",
      "    },\n",
      "    {\n",
      "      \"Heading\": \"Dramatic Finish\",\n",
      "      \"Body\": \"With 15 runs needed off the last over, all eyes were on the batsmen. It was a nail-biting finish, with boundaries and singles being scored with precision. In the end, India managed to score the required runs off the last ball, clinching a thrilling victory over Australia.\"\n",
      "    },\n",
      "    {\n",
      "      \"Heading\": \"Conclusion\",\n",
      "      \"Body\": \"The match between India and Australia was a rollercoaster of emotions, with twists and turns at every corner. Both teams gave it their all, but in the end, it was India who came out on top. Fans were left exhilarated and eager for the next encounter between these two cricketing giants.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "  ],\n",
    "  response_format={ \"type\": \"json_object\" }\n",
    ")\n",
    "\n",
    "print((response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility of Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia won their first Cricket World Cup in 1987. They defeated England in the final held at Eden Gardens in Kolkata, India.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable in the field of Cricket.\"},\n",
    "    {\"role\": \"user\", \"content\": \"When did Australia win their first Cricket World Cup?\"}\n",
    "  ],\n",
    "  seed=42\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color: orange\"><b>Tokens are the fundamental units of NLP</b></span>\n",
    "\n",
    "<span style=\"font-size: 16px; color: blue\"><b>These units are typically words, punctuation marks, or other meaningful substrings that make up the text</b></span>\n",
    "\n",
    "Counting the number of tokens becomes important because - \n",
    "- Number of Tokens determine the amount of computation required and hence the cost you incur both in terms of money and the latency\n",
    "- Context Window or the maximum number of tokens an LLM can process in one go is limited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "####num_tokens_from_string function to count number of tokens in a text string\n",
    "####uses tiktoken to count number of tokens in a text string\n",
    "####parameters: \"string\" is the text string, \"encoding_name\" is the encoding name to be used by tiktoken\n",
    "####returns: num_tokens->number of tokens in the text string\n",
    "####This function is used within extract_data, extract_page, extract_YT, extract_audio, extract_image functions\n",
    "def num_tokens_from_string(string: str, encoding_name=\"cl100k_base\") -> int: #### Function to count number of tokens in a text string ####\n",
    "    encoding = tiktoken.get_encoding(encoding_name) #### Initialize encoding ####\n",
    "    return len(encoding.encode(string)) #### Return number of tokens in the text string ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"Hello! how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38680"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../Assets/Data/alice_in_wonderland.txt\") as f:\n",
    "    AliceInWonderland = f.read()\n",
    "\n",
    "num_tokens_from_string(AliceInWonderland)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PRICING__\n",
    "\n",
    "__gpt-3.5-turbo-0125__\t    |  PROMPT - $0.50 / 1M tokens   |   RESPONSE - $1.50 / 1M tokens\n",
    "\n",
    "__gpt-4__\t                |   PROMPT - $30.00 / 1M tokens\t|   RESPONSE - $60.00 / 1M tokens\n",
    "\n",
    "__gpt-4-turbo__             |\tPROMPT - $10.00 / 1M tokens\t|   RESPONSE - $30.00 / 1M tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! We're at the end of Day 1!\n",
    "\n",
    "Hopefully, now we are fairly confident around using the Chat Completions API and generating text using OpenAI models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/That’s all for the day!.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Assets/Images/profile.png\" width=100> \n",
    "\n",
    "#### Hi! I'm Abhinav! A data science and AI professional with over 15 years in the industry. Passionate about AI advancements, I constantly explore emerging technologies to push the boundaries and create positive impacts in the world. Let’s build the future, together!\n",
    "\n",
    "<span style=\"font-size: 20px; color: orange\"><b>Connect with me!</b></span>\n",
    "\n",
    "\n",
    "[![GitHub followers](https://img.shields.io/badge/Github-000000?style=for-the-badge&logo=github&logoColor=black&color=orange)](https://github.com/abhinav-kimothi)\n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-000000?style=for-the-badge&logo=linkedin&logoColor=orange&color=black)](https://www.linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=abhinav-kimothi)\n",
    "[![Medium](https://img.shields.io/badge/Medium-000000?style=for-the-badge&logo=medium&logoColor=black&color=orange)](https://medium.com/@abhinavkimothi)\n",
    "[![Insta](https://img.shields.io/badge/Instagram-000000?style=for-the-badge&logo=instagram&logoColor=orange&color=black)](https://www.instagram.com/akaiworks/)\n",
    "[![Mail](https://img.shields.io/badge/email-000000?style=for-the-badge&logo=gmail&logoColor=black&color=orange)](mailto:abhinav.kimothi.ds@gmail.com)\n",
    "[![X](https://img.shields.io/badge/Follow-000000?style=for-the-badge&logo=X&logoColor=orange&color=black)](https://twitter.com/abhinav_kimothi)\n",
    "[![Linktree](https://img.shields.io/badge/Linktree-000000?style=for-the-badge&logo=linktree&logoColor=black&color=orange)](https://linktr.ee/abhinavkimothi)\n",
    "[![Gumroad](https://img.shields.io/badge/Gumroad-000000?style=for-the-badge&logo=gumroad&logoColor=orange&color=black)](https://abhinavkimothi.gumroad.com/)\n",
    "\n",
    "\n",
    "<span style=\"font-size: 20px; color: orange\"><b>You can also book a time-slot with me</b></span>\n",
    "\n",
    "[![Static Badge](https://img.shields.io/badge/Free%20Virtual%20Coffee%20(15%20min)-000000?style=for-the-badge&logo=googlecalendar&logoColor=black&color=blue)](https://topmate.io/abhinav_kimothi/544386)\n",
    "[![Static Badge](https://img.shields.io/badge/Resume%20Review%20(DS/AI/ML)%20(30%20min)-000000?style=for-the-badge&logo=googlecalendar&logoColor=blue&color=black)](https://topmate.io/abhinav_kimothi/544382)\n",
    "[![Static Badge](https://img.shields.io/badge/AIML%20Learning%20Path%20(30%20min)-000000?style=for-the-badge&logo=googlecalendar&logoColor=black&color=blue)](https://topmate.io/abhinav_kimothi/544380)\n",
    "[![Static Badge](https://img.shields.io/badge/Generative%20AI%20Consulting%20(60%20min)-000000?style=for-the-badge&logo=googlecalendar&logoColor=blue&color=black)](https://topmate.io/abhinav_kimothi/544379)\n",
    "\n",
    "\n",
    "<span style=\"font-size: 20px; color: orange\"><b>Also, read my ebooks for more on Generative AI!</b></span>\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://abhinavkimothi.gumroad.com/l/GenAILLM\">\n",
    "    <img src=\"https://public-files.gumroad.com/jsdnnne2gnhu61f6hrdprwx2255i\" width=150>\n",
    "</a><a href=\"abhinavkimothi.gumroad.com/l/RAG\">\n",
    "    <img src=\"https://public-files.gumroad.com/v17k9tp2fnbbtg8iwoxt4m3xgivq\" width=150>\n",
    "</a><a href=\"abhinavkimothi.gumroad.com/l/GenAITaxonomy\">\n",
    "    <img src=\"https://public-files.gumroad.com/a730ysxb7a928bb5xkz6fuqabaqp\" width=150>\n",
    "</a>\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
